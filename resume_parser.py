# -*- coding: utf-8 -*-
"""resume_parser_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eGUPVLP83ElUTwijfnFrtFMFufNPudLc

  !apt install -y poppler-utils
"""


import os
import re
#from paddleocr import PaddleOCR
import tempfile
from PIL import Image
from pdf2image import convert_from_path
# from PyPDF4 import PdfFileReader
# from PyPDF4.utils import PdfReadError
import cv2
import numpy as np
from ultralytics import YOLO
from PIL import Image
#import requests
#from io import BytesIO
#import imageio
# from google.colab.patches import cv2_imshow
import easyocr
import json
#import pandas as pd
#from tqdm import tqdm
import spacy
from spacy.tokens import DocBin
from datetime import datetime
import shutil
import string
from pymongo.mongo_client import MongoClient
from pymongo.server_api import ServerApi
import pymongo
from pymongo import MongoClient



#file process and ROIs extraction
class ImageProcessor:
    def __init__(self, input_path, output_dir):
        self.input_path = input_path
        self.output_dir = output_dir
        self.template_folder_path = "demo_v0/templates"
        self.path_model_ROIs = "demo_v0/YOLOv8/best.pt"  # Path to the YOLO model

    def combine_images(self, images, orientation='horizontal'):
        if orientation == 'horizontal':
            width = sum(img.width for img in images)
            height = max(img.height for img in images)
        elif orientation == 'vertical':
            width = max(img.width for img in images)
            height = sum(img.height for img in images)
        else:
            raise ValueError("Invalid orientation. Must be 'horizontal' or 'vertical'.")

        combined_image = Image.new('RGB', (width, height))

        if orientation == 'horizontal':
            x_offset = 0
            for img in images:
                combined_image.paste(img, (x_offset, 0))
                x_offset += img.width
        elif orientation == 'vertical':
            y_offset = 0
            for img in images:
                combined_image.paste(img, (0, y_offset))
                y_offset += img.height

        return combined_image

    def convert_pdf_to_image(self, file_path):
        image_extensions = ('.jpg', '.jpeg', '.png', '.gif')
        if file_path.lower().endswith(image_extensions):
            return file_path

        if not file_path.lower().endswith('.pdf'):
            print("Invalid file format. Only PDFs and images are supported.")
            return None

        try:
            with tempfile.TemporaryDirectory() as temp_dir:
                images = convert_from_path(file_path, output_folder=temp_dir)

                if len(images) > 1:
                    combined_image = self.combine_images(images, orientation='vertical')
                    output_path = os.path.splitext(file_path)[0] + '.jpg'
                    combined_image.save(output_path)
                    return output_path
                elif len(images) == 1:
                    image_path = os.path.splitext(file_path)[0] + '.jpg'
                    images[0].save(image_path)
                    return image_path

        except Exception as e:
            print(f"Failed to convert PDF file: {str(e)}")

        return None

    def is_resume(self, input_image_path, template_folder_path, threshold=0.5):
        input_img = cv2.imread(input_image_path)
        input_img_gray = cv2.cvtColor(input_img, cv2.COLOR_BGR2GRAY)
        input_hist = cv2.calcHist([input_img_gray], [0], None, [256], [0,256])

        for filename in os.listdir(template_folder_path):
            if filename.endswith(".jpg") or filename.endswith(".png"):
                template = cv2.imread(os.path.join(template_folder_path, filename))
                template_gray = cv2.cvtColor(template, cv2.COLOR_BGR2GRAY)
                template_hist = cv2.calcHist([template_gray], [0], None, [256], [0,256])
                similarity = cv2.compareHist(input_hist, template_hist, cv2.HISTCMP_CORREL)
                if similarity > threshold:
                    return 1
        return 0

    def detect_ROIs_and_save_predictions(self, resume_file, model):
        image = cv2.imread(resume_file)
        results = model.predict(image)
        model.predict(source=image, save=True, save_crop=True)

    def move_images_with_new_names(self, source_dir, target_dir):
        os.makedirs(target_dir, exist_ok=True)
        for folder_name in os.listdir(source_dir):
            folder_path = os.path.join(source_dir, folder_name)
            if os.path.isdir(folder_path):
                for file_name in os.listdir(folder_path):
                    file_path = os.path.join(folder_path, file_name)
                    new_file_name = f"{folder_name}_.jpg"
                    new_file_path = os.path.join(target_dir, new_file_name)
                    os.rename(file_path, new_file_path)

    def process_image(self):
        image_path = self.convert_pdf_to_image(self.input_path)
        image = Image.open(image_path)

        if image is not None:
            image.show()

            is_cv = self.is_resume(image_path, self.template_folder_path, threshold=0.7)

            if is_cv == 1:
                print("The image loaded is a resume/cv.")

                is_match = self.is_resume(image_path, self.template_folder_path, threshold=0.3)
                if is_match == 1:
                    print("The cv matches our templates. Good parsing.")
                else:
                    print("The template is new. Need to add it to templates for better results.")

                model1 = YOLO(self.path_model_ROIs)
                path_image = image_path
                self.detect_ROIs_and_save_predictions(path_image, model1)
                self.move_images_with_new_names("runs/detect/predict5/crops", self.output_dir)
                folder_path = self.output_dir
            else:
                print("The input image is not a resume/cv image.")
        else:
            print("Failed to convert the file to an image.")

import os
import cv2
from PIL import Image
#experiences extraction from the experience image
class ExperiencesExtraction:
    def __init__(self, input_dir, output_experience_dir, word_to_find):
        self.input_dir = input_dir
        self.output_experience_dir = output_experience_dir
        self.word_to_find = word_to_find
        self.path_model_experience = "demo_v0/experience detection/best (1).pt"

    def find_image_with_word_in_name(self, folder_path, word):
        for filename in os.listdir(folder_path):
            if word in filename.lower() and filename.lower().endswith(('.jpg', '.jpeg', '.png', '.gif', '.bmp')):
                return os.path.join(folder_path, filename)

        return None  # Return None if the image with the specified word is not found

    def detect_experience_and_save_predictions(self, experience_file, model):
        image = cv2.imread(experience_file)
        results = model.predict(image)
        model.predict(source=image, save=True, save_crop=True)

    def move_images_with_new_names(self, source_dir, target_dir):
        os.makedirs(target_dir, exist_ok=True)

        for folder_name in os.listdir(source_dir):
            folder_path = os.path.join(source_dir, folder_name)
            if os.path.isdir(folder_path):
                image_counter = 1
                for file_name in os.listdir(folder_path):
                    file_path = os.path.join(folder_path, file_name)
                    new_file_name = f"{folder_name}{image_counter}_.jpg"
                    image_counter += 1
                    new_file_path = os.path.join(target_dir, new_file_name)
                    os.rename(file_path, new_file_path)

    def extract_experiences(self):
        image_path = self.find_image_with_word_in_name(self.input_dir, self.word_to_find)
        if image_path:
            image = Image.open(image_path)
            if image is not None:
                image.show()  # Display the image

                model = YOLO(self.path_model_experience)
                self.detect_experience_and_save_predictions(image_path, model)
                self.move_images_with_new_names("/content/runs/detect/predict6/crops", self.output_experience_dir)

        return self.output_experience_dir



# ocr text  extraction from ROIs in a json file and date processing
class OCR_TEXT_EXTRACTION:
    def __init__(self, src_folder1, src_folder2, dest_folder):
        self.src_folder1 = src_folder1
        self.src_folder2 = src_folder2
        self.dest_folder = dest_folder

    def combine_folders(self):
        # Create the destination folder if it doesn't exist
        if not os.path.exists(self.dest_folder):
            os.makedirs(self.dest_folder)

        # Get the list of files in the source folders
        files_folder1 = os.listdir(self.src_folder1)
        files_folder2 = os.listdir(self.src_folder2)

        # Combine files from folder1 to the destination folder
        for file in files_folder1:
            src_file = os.path.join(self.src_folder1, file)
            dest_file = os.path.join(self.dest_folder, file)
            shutil.copy(src_file, dest_file)

        # Combine files from folder2 to the destination folder
        for file in files_folder2:
            src_file = os.path.join(self.src_folder2, file)
            dest_file = os.path.join(self.dest_folder, file)
            shutil.copy(src_file, dest_file)

    def extract_text_from_images(self):
        # Create an empty dictionary to store the extracted text
        output_dict = {}

        # Create the EasyOCR Reader object with French language
        reader = easyocr.Reader(['fr'], gpu=False)

        # Iterate over all the image files in the directory
        for filename in os.listdir(self.dest_folder):
            if filename.endswith('.jpg') or filename.endswith('.JPEG') or filename.endswith('.jpeg'):
                # Read the image file
                img_path = os.path.join(self.dest_folder, filename)
                img = cv2.imread(img_path)

                # Extract the text from the image
                results = reader.readtext(img, detail=1, paragraph=False)

                # Extract the text from the results and concatenate it into a single string
                extracted_text = ' '.join([text for (_, text, _) in results])

                # Extract the skill name from the filename
                skill_name = os.path.splitext(filename)[0]
                if skill_name in output_dict:
                    output_dict[skill_name].append(extracted_text)
                else:
                    output_dict[skill_name] = [extracted_text]

        # Save the output dictionary to a JSON file
        output_file = os.path.join(self.dest_folder, 'EXTRACTED_text.json')
        with open(output_file, 'w') as f:
            json.dump(output_dict, f, indent=4)

        # Return the output dictionary
        print("text extraction completed!")
        return output_dict

    @staticmethod
    def convert_short_months(json_path):
        # Define a dictionary to map short month names to full month names
        month_mapping = {
            'jan': 'january',
            'feb': 'february',
            'mar': 'march',
            'apr': 'april',
            'may': 'may',
            'jun': 'june',
            'jul': 'july',
            'aug': 'august',
            'sep': 'september',
            'oct': 'october',
            'nov': 'november',
            'dec': 'december'
        }

        # Read the JSON file
        with open(json_path, 'r') as file:
            data = json.load(file)

        # Convert short month names to full month names
        for key, value in data.items():
            if isinstance(value, list):
                updated_values = []
                for item in value:
                    # Find short month names using regex
                    matches = re.findall(r'\b[a-zA-Z]{3}\b', item)
                    for match in matches:
                        # Replace short month name with full month name
                        if match.lower() in month_mapping:
                            item = item.replace(match, month_mapping[match.lower()])
                    updated_values.append(item)
                data[key] = updated_values

        # Save the changes back to the JSON file
        with open(json_path, 'w') as file:
            json.dump(data, file, indent=4)

    @staticmethod
    def convert_dates_in_json_file(file_path):
        # Load the JSON data from the file
        with open(file_path, 'r') as file:
            data = json.load(file)

        # Regular expression pattern to find dates in the format "month year" or "month day, year"
        date_pattern = r'(\b[a-zA-Z]+\b(?:\s+\d{1,2},)?\s+\d{4})'

        # Function to convert date format from "month year" or "month day, year" to "dd-mm-yyyy"
        def convert_date(match):
            old_date = match.group(1)
            # Determine the date format and parse accordingly
            try:
                date_obj = datetime.strptime(old_date, '%B %d, %Y')
            except ValueError:
                try:
                    date_obj = datetime.strptime(old_date, '%B %Y')
                except ValueError:
                    # If the date format does not match either pattern, return the original date as it is
                    return old_date

            new_date = date_obj.strftime('%d-%m-%Y')
            return new_date

        # Update the dates in the JSON data
        for key, value in data.items():
            if isinstance(value, list):
                for i, item in enumerate(value):
                    value[i] = re.sub(date_pattern, convert_date, item)

        # Save the changes back to the same file
        with open(file_path, 'w') as file:
            json.dump(data, file, indent=4)

    @staticmethod
    def process_json_file(file_path):
        def convert_dates_form(input_text):
            converted_text = ""
            lines = input_text.split(".")
            for line in lines:
                line = line.strip()
                words = line.split()
                for i in range(len(words)):
                    try:
                        date = datetime.strptime(words[i], '%d/%m/%Y')
                        words[i] = datetime.strftime(date, '%d-%m-%Y')
                    except ValueError:
                        pass

                    try:
                        date = datetime.strptime(words[i], '%d-%m-%Y')
                        words[i] = datetime.strftime(date, '%d-%m-%Y')
                    except ValueError:
                        pass

                converted_text += " ".join(words) + ". "

            return converted_text

        with open(file_path, 'r+') as file:
            json_data = json.load(file)
            processed_data = {}

            for key, value in json_data.items():
                if isinstance(value, list):
                    processed_values = []
                    for item in value:
                        processed_values.append(convert_dates_form(item))
                    processed_data[key] = processed_values

            file.seek(0)
            json.dump(processed_data, file, indent=4)
            file.truncate()

    def process_and_return_json(self):
        self.combine_folders()
        extracted_text = self.extract_text_from_images()
        json_path = os.path.join(self.dest_folder, 'EXTRACTED_text.json')
        self.convert_short_months(json_path)
        self.convert_dates_in_json_file(json_path)
        self.process_json_file(json_path)
        print("Date Conversion completed!")
        return extracted_text



#regions segmentation and generation of final json file
class ROI_SEGMENTATION:
    def __init__(self, ROIs_DIR, EXPERIENCES_DIR, NLP_NER_MODEL):
        self.ROIs_DIR = ROIs_DIR
        self.EXPERIENCES_DIR = EXPERIENCES_DIR
        self.NLP_NER = spacy.load(NLP_NER_MODEL)

    def delete_key_from_json(self, json_file_path, key_name):
        try:
            with open(json_file_path, 'r') as file:
                data = json.load(file)

            if key_name in data:
                del data[key_name]

                with open(json_file_path, 'w') as file:
                    json.dump(data, file, indent=4)
                print(f"The key '{key_name}' was successfully deleted from the JSON file.")
            else:
                print(f"The key '{key_name}' does not exist in the JSON file.")

        except FileNotFoundError:
            print(f"File '{json_file_path}' not found.")
        except json.JSONDecodeError:
            print(f"Error decoding JSON in '{json_file_path}'.")
        except Exception as e:
            print(f"An error occurred: {e}")

    def extract_contact_info(self, json_file):
        with open(json_file) as file:
            data = json.load(file)

        name_key = next((key for key in data.keys() if 'name' in key.lower()), None)
        name_info = data.get(name_key, []) if name_key else []
        name_text = ' '.join(name_info)
        name_match = re.findall(r'([A-Za-z\s-]+)', name_text)
        name = name_match if name_match else None

        contact_key = next((key for key in data.keys() if 'contact' in key.lower()), None)
        contact_info = data.get(contact_key, []) if contact_key else []
        contact_text = ' '.join(contact_info)

        phone_match = re.findall(r'\+\d{2}\s\d{2}\s\d{3}\s\d{3,4}', contact_text)
        phone_number = phone_match if phone_match else None

        email_match = re.findall(r'\S+@\S+', contact_text)
        email_address = email_match if email_match else None

        linkedin_match = re.findall(r'(?:https?://)?(?:www\.)?linkedin\.com/\S+', contact_text)
        linkedin_url = linkedin_match if linkedin_match else None

        extracted_info = {
            "personal details": {
                "Name": name,
                "Phone Number": phone_number,
                "Email Address": email_address,
                "LinkedIn URL": linkedin_url
            }
        }

        extracted_details_path = os.path.join(self.ROIs_DIR, "Personal_details.json")
        with open(extracted_details_path, 'w') as outfile:
            json.dump(extracted_info, outfile, indent=4)

        return extracted_details_path

    def detect_and_save_experience_classes(self, json_file_path, output_dir):
        def find_experience_keys(json_data):

          experience_keys = []
          for key in json_data.keys():
              if 'experience' in key.lower():
                  experience_keys.append(key)
          return experience_keys

        def save_detected_classes_to_json(doc, output_file, key_name):
           experiences = {}
           for ent in doc.ents:
               label = ent.label_
               text = ent.text
               if label not in experiences:
                   experiences[label] = [text]
               else:
                   experiences[label].append(text)
           with open(output_file, 'w') as json_file:
               json.dump({key_name: experiences}, json_file, indent=4)


        with open(json_file_path, 'r') as json_file:
            json_content = json.load(json_file)

        experience_keys = find_experience_keys(json_content)

        if experience_keys:
            os.makedirs(output_dir, exist_ok=True)

            for idx, experience_key in enumerate(experience_keys, start=1):
                text_list = json_content.get(experience_key, [])
                if isinstance(text_list, list):
                    text = ' '.join(text_list)
                    doc = self.NLP_NER(text)
                    output_file = os.path.join(output_dir, f"experience{idx}_classes.json")
                    save_detected_classes_to_json(doc, output_file, f"experience{idx}")
                else:
                    print(f"Invalid text format for key '{experience_key}'. Expected a list in the JSON file.")
        else:
            print("No experience key found in the JSON file.")

    def combine_json_files(self, folder_path, output_file_name):
        combined_data = {}
        for root, _, files in os.walk(folder_path):
            for file in files:
                if file.endswith(".json"):
                    file_path = os.path.join(root, file)
                    with open(file_path, "r") as f:
                        data = json.load(f)
                        combined_data.update(data)

        output_file_path = os.path.join(folder_path, f"{output_file_name}.json")
        with open(output_file_path, "w") as f:
            json.dump(combined_data, f, indent=4)

    def process_roi_segmentation(self):
        json_to_process = os.path.join(self.ROIs_DIR, 'EXTRACTED_text.json')
        key_name_to_delete = 'Experience_'
        self.delete_key_from_json(json_to_process, key_name_to_delete)

        file_name = os.path.join(self.ROIs_DIR, 'all_experiences')
        self.detect_and_save_experience_classes(json_to_process, self.EXPERIENCES_DIR)
        extracted_personal_details = self.extract_contact_info(json_to_process)

        self.combine_json_files(self.EXPERIENCES_DIR, file_name)

        return json_to_process, file_name + '.json', extracted_personal_details

#cleaning the final json file
class CLEANING_PROCESS:
    def __init__(self, rois_file, experience_file, personal_details_file):
        self.rois_file = rois_file
        self.experience_file = experience_file
        self.personal_details_file = personal_details_file

    def _delete_key_with_specific_word(self, json_file_path, key_name_keyword):
        try:
            with open(json_file_path, 'r') as file:
                data = json.load(file)

            keys_to_delete = [key for key in data if key_name_keyword in key]
            for key in keys_to_delete:
                del data[key]

            with open(json_file_path, 'w') as file:
                json.dump(data, file, indent=4)

        except FileNotFoundError as e:
            print(f"Error: {e}. Make sure the file exists.")
        except json.JSONDecodeError as e:
            print(f"Error decoding JSON: {e}")
        except Exception as e:
            print(f"An error occurred: {e}")

    def _combine_multiple_json_files(self, input_file_paths, output_file_path):
        combined_data = {}

        for file_path in input_file_paths:
            if file_path.endswith(".json"):
                with open(file_path, "r") as f:
                    try:
                        data = json.load(f)
                        if isinstance(data, dict):
                            for key, value in data.items():
                                if key in combined_data:
                                    if not isinstance(combined_data[key], list):
                                        combined_data[key] = [combined_data[key]]
                                    combined_data[key].append(value)
                                else:
                                    combined_data[key] = value
                        else:
                            print(f"Warning: {file_path} does not contain a valid JSON object (dictionary). Skipping...")
                    except json.JSONDecodeError as e:
                        print(f"Error reading {file_path}: {e}. Skipping...")

        if not combined_data:
            print("No valid JSON data found in the specified files.")
            return

        with open(output_file_path, "w") as f:
            json.dump(combined_data, f, indent=4)

    def _remove_unwanted_words_from_json(self, file_path, unwanted_words):
        with open(file_path, 'r') as file:
            json_data = json.load(file)

        def process_value(value):
            if isinstance(value, list):
                return [process_value(v) for v in value]
            elif isinstance(value, dict):
                return {k: process_value(v) for k, v in value.items()}
            elif isinstance(value, str):
                for word in unwanted_words:
                    value = value.replace(word, '').strip()
                return ' '.join(value.split())
            else:
                return value

        processed_data = process_value(json_data)

        with open(file_path, 'w') as file:
            json.dump(processed_data, file, indent=4)

    def _remove_punctuation_from_json_file(self, file_path, keys_to_process):
        with open(file_path, 'r') as file:
            json_data = json.load(file)

        for key in keys_to_process:
            if key in json_data:
                values = json_data[key]
                for i in range(len(values)):
                    translator = str.maketrans('', '', string.punctuation.replace('-', ''))
                    values[i] = values[i].translate(translator).strip()

        with open(file_path, 'w') as file:
            json.dump(json_data, file, indent=4)

    def clean_and_combine(self, final_file_path):
        key_name_keyword = 'experience'
        unwanted_words = ['Experience', 'Education', 'hello', 'skills', 'contact', 'social life']
        keys_to_process = ["Education_", "Name_"]

        # Step 1: Delete keys containing the word 'experience'
        self._delete_key_with_specific_word(self.rois_file, key_name_keyword)

        # Step 2: Combine all JSON files into a single file
        input_files = [self.rois_file, self.experience_file, self.personal_details_file]
        self._combine_multiple_json_files(input_files, final_file_path)

        # Step 3: Clean the final JSON file
        # Remove unwanted words
        self._remove_unwanted_words_from_json(final_file_path, unwanted_words)

        # Remove unwanted punctuation
        self._remove_punctuation_from_json_file(final_file_path, keys_to_process)

        print(f"Cleaning and combining process completed. Final JSON file saved to {final_file_path}")

import json

class ResumeParser:
    def __init__(self, input_path):
        self.INPUT_PATH = input_path
        self.OUTPUT_DIR = "Final_Result"
        self.INPUT_DIR="Final_Result"
        self.ROIs_folder_path="Final_Result"
        self.OUTPUT_EXPERIENCE_DIR = "Experienses"
        self.DESTINATION_FOLDER = "All_ROIs"
        self.ROIs_DIR="All_ROIs"
        self.EXPERIENCES_DIR="All_ROIs/Experiences"
        self.NLP_NER_MODEL = "model-best"
        self.FINAL_FILE = "MAIN_FILE.json"

    def parse_resume(self):
#image process and ROIS extraction
        processor = ImageProcessor(self.INPUT_PATH, self.OUTPUT_DIR)
        processor.process_image()
######################
# Experiences Extraction

        WORD_TO_FIND = "experience"

        experiences_extractor = ExperiencesExtraction(self.INPUT_DIR, self.OUTPUT_EXPERIENCE_DIR, WORD_TO_FIND)
        extracted_folder = experiences_extractor.extract_experiences()
        print("Experiences extracted and saved in:", extracted_folder)
##########################
## OCR Text Extraction

        experience_folder_path = self.OUTPUT_EXPERIENCE_DIR


        ocr_extractor = OCR_TEXT_EXTRACTION(self.ROIs_folder_path, experience_folder_path, self.DESTINATION_FOLDER)
        extracted_text = ocr_extractor.process_and_return_json()
##################
#ROIs segmentation


        roi_segmentation = ROI_SEGMENTATION(self.ROIs_DIR, self.EXPERIENCES_DIR, self.NLP_NER_MODEL)
        extracted_text_json, all_experiences_json, extracted_personal_details_json = roi_segmentation.process_roi_segmentation()

        print("EXTRACTED_text.json:", extracted_text_json)
        print("All_experiences.json:", all_experiences_json)
        print("Personal_details.json:", extracted_personal_details_json)

#############################
## Data Cleaning and Combination

        ROIs_FILE = extracted_text_json
        EXPERIENCE_FILE = all_experiences_json
        PERSONAL_DETAILS_FILE = extracted_personal_details_json



        cleaner = CLEANING_PROCESS(ROIs_FILE, EXPERIENCE_FILE, PERSONAL_DETAILS_FILE)
        cleaner.clean_and_combine(self.FINAL_FILE)

        return self.FINAL_FILE

    def display_output(self, final_file_path):
        with open(final_file_path, 'r') as file:
            data = json.load(file)
            print(json.dumps(data, indent=4))
    
    def data_output(self, final_file_path):
        with open(final_file_path, 'r') as file:
            data = json.load(file)
            return data

#if __name__ == "__main__":
 #   input_path = "demo_v0/templates/Sahnoun_Ismail_CV.jpg"
  #  resume_parser = ResumeParser(input_path)
   # final_file_path = resume_parser.parse_resume()
    #print("Final output file:", final_file_path)
    #resume_parser.display_output(final_file_path)

import json
from pymongo import MongoClient

class MongoDBLoader:
    def __init__(self, database_name, collection_name, uri):
        self.client = MongoClient(uri)
        self.db = self.client[database_name]
        self.collection = self.db[collection_name]

    def load_json_to_mongodb(self, json_file_path):
        """
        Load a JSON file into the MongoDB collection.
        """
        try:
            with open(json_file_path, 'r') as file:
                data = json.load(file)
                self.collection.insert_one(data)
            print("JSON data loaded into MongoDB successfully.")
        except FileNotFoundError:
            print("The specified JSON file does not exist.")
        except Exception as e:
            print("An error occurred while loading JSON data into MongoDB:", str(e))

if __name__ == "__main__":
    input_path = "demo_v0/templates/Sahnoun_Ismail_CV.jpg"
    resume_parser = ResumeParser(input_path)
    final_file_path = resume_parser.parse_resume()

    if final_file_path:
        print("Final output file:", final_file_path)

        # Initialize MongoDBLoader with your preferred database, collection names, and MongoDB Atlas URI

        database_name = "resume_data"
        collection_name = "resume_collection"
        uri = "mongodb+srv://kadhizayneb:QUj5PXn19KUjoU91@resume-parser-cluster.8alehr1.mongodb.net/?retryWrites=true&w=majority"
        loader = MongoDBLoader(database_name, collection_name, uri)

        # Load the final output JSON file into MongoDB
        loader.load_json_to_mongodb(final_file_path)

